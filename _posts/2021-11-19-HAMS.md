# Revamping Storage Class Memory With Hardware Automated Memory-Over-Storage Solution

## 摘要

像NVDIMM这样的大容量持久存储器一直被认为是一种开创性的存储器技术，因为它们即使在断电后也能维持系统的状态，并允许系统快速恢复。然而，由大量的软件栈干预引起的开销严重地抵消了这种内存的好处。首先，为了显著减少软件堆栈开销，我们提出了HAMS，一种硬件自动化的存储上内存(MoS)解决方案。具体来说，HAMS将NVDIMM和超低延迟flash卷(ULL-Flash)的容量聚集到一个大的内存空间中，这些内存空间可以作为工作内存扩展或持久内存扩展，以一种os透明的方式使用。HAMS位于内存控制器集线器中，并通过传统的DDR和NVMe接口管理其MoS地址池;它使用一个简单的硬件缓存，将ull-flash的存储空间映射到NVDIMM的内存空间后，服务于主机MMU的所有内存请求。其次，为了提高HAMS的效率和可靠性，我们提出了一种“高级HAMS”，通过优化HAMS的数据路径和硬件模块，消除NVDIMM和ull-flash之间不必要的数据传输。这种方法从存储盒释放ull-flash及其NVMe控制器，并通过传统的DDR4接口直接将HAMS数据路径连接到NVDIMM。我们的评估显示，与基于软件的NVDIMM设计相比，HAMS和高级HAMS可以提供97%和119%的系统性能，而能耗分别降低41%和45%。

## 1. 引言

最近，诸如PRAM[41]和3D XPoint[48]等持久性存储器受到了相当大的关注，因为它们的固有非易失性、高密度和低功耗可以使现代数据中心和高性能计算机受益。对于这样的系统，需要后端存储来从系统故障和崩溃中恢复。由于持久内存可以自发地瞬时恢复所有内存状态，它们可以消除对后端存储的大量访问和相关的运行时开销[38]，[49]，[61]。此外，企业工作站和服务器使用DirectAccess (DAX)[18]的持久内存[67]，这带来了前所未有的性能水平和数据弹性[57]的优势。

有三种标准的持久内存类型(即NVDIMM-N/F/P)。NVDIMM-F将flash直接集成到DIMM (dual-inline memory module)中，提供类似存储的高容量。但是，NVDIMM-F不能简单地代替DRAM，因为它只暴露了一个块接口。NVDIMM-P如Optane DC PMM是字节可寻址的，但它使用应用程序直接模式来支持数据持久性的性能仍然是6×worse比DRAM[26]，[29]。相反，NVDIMM-N的目标是提供“字节可寻址”的持久性，具有类似于dram的性能[54]。NVDIMM-N通常由一个小型闪存设备和多个带有电池的DRAM模块组成。NVDIMM-N可用于广泛的数据密集型应用，如数据库管理系统(DBMS)[2]、事务处理[45]、[63]、数据分析[8]和检查点[12]。但nvdimm - n (4GB ~ 64GB)的内存空间比NVDIMM-P和ssd等持久性存储设备要小得多。此外，NVDIMM-Ns中的DRAM容量受到电池规模不佳的限制，当电源故障发生[5]时，需要为DRAM备份操作提供电源，[71]。例如，在过去的二十年里，DRAM的存储密度增加了许多数量级，而锂离子电池的能量密度只增加了三倍[34]。

要构建一个大型的、可扩展的、持久的内存空间，一个可能的解决方案是将NVDIMM-N与SSD和内存映射文件(MMFs)一起使用，这些文件可以在操作系统内存管理器或文件系统中实现。这允许数据密集型应用程序使用传统的负载/存储指令访问大的存储空间。然而，我们观察到，与仅使用nvdimm-n的解决方案相比，这种mmf辅助的持久内存在用户级平均会降低48%的应用程序性能(参见第III-B节)。这种严重的性能下降不仅是由于访问SSD时的长失速延迟，而且在传统存储堆栈中，软件开销和用户和系统内存空间之间频繁的数据复制。

![](/images/HAMS/NVDIMM_HAMS.png)

为了解决上述限制，我们提出了HAMS，一种硬件自动存储(MoS)解决方案，它聚合了NVDIMMN的内存容量和新的超低延迟flash存档的存储容量，被称为ull-flash [37]， [68]，集成为一个大内存空间(见图1)。HAMS的大单片内存空间可以用作工作内存或持久性内存扩展。我们的HAMS位于内存控制器集线器中，并通过利用传统的DDR4和NVMe接口来管理其MoS地址池。为此，通过将ull-flash的存储空间映射到NVDIMM-N的内存空间，HAMS使用了一个简单的硬件缓存来处理来自主机内存管理单元(MMU)的所有内存请求。在NVDIMM-N缓存缺失的情况下，HAMS在内部管理NVMe命令和I/O请求队列，同时隐藏操作系统的所有NVMe协议和接口管理开销，这样MMU请求的数据总是由NVDIMM-N提供。

虽然HAMS的“基线”设计可以提供20GB/s的峰值带宽，但它仍然可以产生次优的系统性能，特别是在运行大规模数据密集型应用程序时，由于一些效率低下，下文将介绍。首先，处理NVDIMM-N的cache miss需要在NVDIMM-N和ull-flash之间进行数据传输。也就是说，HAMS需要通过DDR4和PCIe接口(包括物理层、控制器和协议管理器)来处理NVDIMM-N缓存丢失。然而，PCIe带宽不足以向HAMS展示ull-flash的全部潜力。因此，对于大型数据密集型应用程序，为了处理频繁的NVDIMM-N缓存丢失而进行的数据传输可能会造成高达47%的HAMS内存访问延迟。其次，一些数据可能冗余地存在于NVDIMM-N和ULLFlash的内部DRAM中，用作缓存和/或缓冲区。例如，大多数现代ssd，包括ULL-Flash，使用大型内部dram来缓冲/缓存所有传入的I/O请求，以隐藏底层flash的长延迟。这将有助于ssd在块存储文件系统中提高性能，但在基于mos的解决方案中使用ssd会浪费电力，并增加ssd的内部复杂性。

为了解决这些局限性，我们还建议通过修改其数据路径和硬件模块，将其积极集成到现有的计算机系统中。就数据持久性而言，这使得基线解决方案更加节能和可靠。这种“高级HAMS”从存储盒中释放ULL-Flash及其NVMe控制器，并将它们的数据路径直接连接到NVDIMM-N。为此，我们建议稍微修改ull-flash中的NVMe控制器，方法是合并一个新的基于寄存器的接口，并将该接口与HAMS的DDR4接口紧密集成。这种积极的集成允许ull-flash访问NVDIMM-N中的DRAM设备，而不需要来自HAMS的任何干预，并从ull-flash中删除DRAM缓冲区，同时启用完整的NVMe功能。

我们的评估结果显示，与基于mmf的NVDIMM-N+SSD混合设计相比，HAMS和高级HAMS提供了97%和119%的系统性能，同时消耗的系统能量分别减少41%和45%。

## 2. 背景

在本节中，我们首先描述持久内存的关键硬件组件和异构内存扩展的存储堆栈。接下来，我们给出了ull-flash的硬件和固件细节。

### 2.A 持久内存与存储

图2描述了一个系统架构的高级视图，其中包括NVDIMM-N/P和ULL-Flash。NVDIMM-N/P通过DDR内存总线连接到MCH (memory controller hub)，作为CPU的标准注册内存(RDIMM)，而ull-flash通过PCIe根complex连接到MCH作为块存储。

![](/images/HAMS/persistent_memory.png)

**持久的记忆**：持久内存有三种标准版本:NVDIMM-N[31]、-F[54]和-P[16]。表1总结了这三种类型的持久内存和我们的设计之间的关键区别。NVDIMM-N是针对持久内存模块的JEDEC标准，它包括DRAM设备、超级电容、多路复用器和小型flash设备。超级电容被用作DRAM备份操作的能源，当电源故障发生时。多路复用器位于DRAM和标准内存连接器到内存总线之间，当发生备份和恢复操作时，它们将DRAM与内存总线隔离。闪存作为备份存储介质，与DRAM具有相同的容量，但对用户是不可见的。当主机直接访问NVDIMM-N的DRAM时，它的控制器在电源故障时内部将DRAM数据迁移到闪存，这种迁移通常需要数十秒[31]。控制器在下一次引导时将数据从闪存恢复到DRAM，从而提供非易变性。相反，NVDIMM-F由多个不带DRAM的闪存组成。由于NVDIMM-F通常用作块存储，因此它需要文件系统和操作系统的支持，类似于传统的ssd。NVDIMM-P结合了NVDIMM-N和NVDIMM-F的设计策略，采用了字节可寻址接口。然而，像Optane DC PMM这样的NVDIMM-P比DRAM表现出6×lower的性能，并且不允许直接访问其内部DRAM，并且需要操作系统级别的支持来实现持久内存访问。到目前为止，这使得NVDIMM-N成为唯一支持具有字节可寻址性的DRAM性能的持久内存。考虑到这一点，在本文中，我们交替使用术语“NVDIMM”和“NVDIMM-n”。

![](/images/HAMS/feature_comparison.png)

**存储**：所有高性能ssd，包括ULLFlash，都连接到MCH的另一部分，PCIe根复合体。在现代计算机系统中，PCIe通道也被视为内存总线，但它在CPU和SSD之间为I/O事务传输4KB或更大的数据包。由于I/O访问的粒度是一个页面或一个块，用户应用程序只能通过操作系统的整个存储堆栈访问底层SSD，其中包括一个NVMe驱动程序之上的I/O运行时库、文件系统和块层。NVMe驱动程序管理PCIe上的数据包传输，并通过PCIebaseline地址寄存器(bar)与SSD中的NVMe控制器通信，这些地址寄存器包括门铃寄存器、队列属性、每个队列的目标地址以及NVMe控制器信息[10]。ull-flash的内部硬件细节将在II-C节中解释。

### 2.B 持久内存扩展的支持

图3展示了用户应用程序将NVDIMM扩展为SSD所需的软件支持和存储堆栈。Linux系统中的MMF (memory-mapped file)模块，也称为mmap，可以用SSD来扩展NVDIMM的持久内存空间。如果一个进程对SSD(1)调用带有文件描述符(fd)的mmap, MMF在它的进程地址空间中创建一个新的映射，由一个内存管理结构(mm_struct)表示，通过分配一个虚拟内存区域(VMA)给该结构(2)。换句话说，MMF连接fd到VMA，通过在进程内存和目标文件之间建立一个映射。当进程访问VMA(345)指定的内存时，会触发页面错误(如果NVDIMM中没有可用的数据)。

![](/images/HAMS/software_support.png)

当发生页面错误时，将调用页面错误处理程序并将一个新页面分配给VMA。由于VMA链接到目标文件，页面错误处理程序检索与fd关联的文件元数据(inode)，为其访问获取一个锁(6)。MMU与文件系统的错误处理程序交互，从SSD读取页面。文件系统初始化一个块I/O请求结构，称为bio，并将其提交给多队列块I/O队列(blkmq)层，该层通过多个软件队列调度I/O请求(7)。队列可以被映射到一个或多个软件硬件调度队列(8),由NVM控制器存在的SSD(9)内一旦I / O请求的服务(例如,生物)完成后,和实际的数据加载到一个新的区域的内存分配页面,页面错误处理程序创建一个页表条目(PTE),在PTE中记录新的页面地址，然后继续该进程。

MMF可以和ssd盘一起扩展NVDIMM的持久内存空间。然而，由于页面错误、文件系统、上下文切换和数据复制造成的高开销，这种方法可能会抵消ull-flash带来的大部分好处。

### 2.C ULL-Flash

**硬件细节**：所有最先进的ssd通常使用大量闪存包，并将它们连接到多个系统总线，即channels。每个flash包包含多个模具和平面，以实现快速响应和低延迟，如图4a所示。为了提供大量的并行性和高I/O性能，SSD将主机上的给定I/O请求分散到多个通道、包、die甚至平面上。

![](/images/HAMS/overview.png)

ull-flash也采用了这种多通道多路架构，但对数据路径和通道剥离[9]进行了优化。更具体地说，ULL-Flash将来自主机的4KB I/O请求拆分为两个操作，并同时将它们发送给两个通道;这样做可以有效地将DMA延迟减少一半。此外，虽然大多数高性能ssd采用多水平单元(MLC)或三水平单元(TLC)，但ULL-Flash采用一种新型的flash介质，称为z-nand[9]。Z-NAND利用3d flash结构提供单层单元(SLC)技术，并优化I/O电路和内存接口，以实现短延迟。具体来说，Z-NAND使用了48个堆叠的字线层，被称为垂直NAND (V-NAND)架构，以体现SLC内存。由于其独特的闪存结构和先进的制造技术，Z-NAND闪存(3 μ s和100 μ s)的读写时延为15×and 7×lower，而V-NAND闪存的读写时延为[55]。

ull-flash在其多个通道前使用大DRAM，并通过NVMe接口公开其内部并行性、低延迟和高带宽，这些都由多个接口控制器和固件模块管理。请注意，DRAM管理与NVMe协议的处理是紧密耦合的。根据NVMe的定义，在底层的ull-flash控制器或固件执行DMA进行数据传输之后，主机端DRAM和ssd内部DRAM中都可以存在相同的数据。

**连接到CPU的I/O**：图4b展示了每核NVMe队列和通信协议。NVMe队列由一对提交队列(SQ)和完成队列(CQ)组成，每个队列都有64K条目[22]。这些是简单的FIFO队列，每个条目都由物理区域页面(PRP)指针[10]引用。如果请求的大小大于一个4KB的NVMe包，则可以通过一组PRP指针而不是单个PRP指针来引用数据。当请求到达SQ时，主机增加其尾部(指针)并按下相应的ull-flash门铃，NVMe控制器就可以同步存储侧SQ，存储侧SQ在逻辑上与主机侧SQ配对。由于每个条目的数据都存在于主机端DRAM中(由PRP指针指向)，因此ull-flash为I/O请求处理DMA，然后底层的Z-NAND和固件为请求服务。一旦服务完成，NVMe控制器移动CQ的尾部(与SQ配对)，并通过消息通知中断(MSI)通知主机事件。然后，主机跳转到中断服务程序(ISR)，并同步CQ尾部。ISR完成请求，推进CQ的头(并释放缓冲区数据)，并按门铃通知ull-flash主机端I/O处理完成。最后，NVMe控制器释放内部数据，推进CQ的头指针。NVMe接口不知道缓存在主机端DRAM中的数据，而每个I/O请求的数据可以驻留在主机端DRAM中。因此，即使主机端DRAM可以处理I/O请求，NVMe接口也会毫不相干地将请求排队并处理它们。

