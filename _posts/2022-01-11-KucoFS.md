---
layout: post
title: Scalable Persistent Memory File System with Kernel-Userspace Collaboration
date: 2022-01-11
category: translation
---

# 摘要

我们介绍了Kuco，一种新颖的直接访问文件系统架构，其主要目标是可伸缩性。Kuco利用三种关键技术——协作索引、两级锁定和版本控制读取——将耗时的任务(如路径名解析和并发控制)从内核转移到用户空间，从而避免了内核处理瓶颈。在Kuco的基础上，我们提出了KucoFS的设计和实现，并通过实验证明了KucoFS在广泛的实验范围内具有良好的性能;重要的是，在元数据操作方面，KucoFS比现有的文件系统有更好的扩展性，可以达到一个数量级，并且可以充分利用设备带宽进行数据操作。

# 1. 引言

新兴的可字节寻址持久存储器(PM)，如PCM、ReRAM和最近发布的Intel Optane DCPMM，提供接近DRAM的性能和类似于磁盘的数据持久性。这种高性能硬件增加了重新设计高效文件系统的重要性。在过去的十年里，系统社区已经提出了许多文件系统，如BPFS、PMFS和NOVA，以最小化由传统文件系统架构引起的软件开销。但是，这些pm感知的文件系统是操作系统的一部分，应用程序需要进入内核才能访问它们，其中系统调用和虚拟文件系统(VFS)仍然会产生不可忽略的开销。在这方面，最近的研究提出在用户空间中部署文件系统直接访问文件数据(即直接访问)，从而利用PM的高性能。

尽管做出了这些努力，我们发现另一个重要的性能指标——可伸缩性——仍然没有得到很好的解决，特别是当多核处理器遇上快速PM时。NOVA通过对内部数据结构进行分区和避免使用全局锁，提高了多核可伸缩性。但是，我们的评估表明，由于VFS层的存在，它仍然不能很好地伸缩。更糟糕的是，一些用户空间文件系统设计通过引入集中式组件进一步加剧了可伸缩性问题。例如，Aerie通过向具有更新元数据权限的可信进程(TFS)发送昂贵的进程间通信(ipc)来确保文件系统元数据的完整性。另一个例子是Strata，它通过直接在PM日志中记录更新，避免了在正常操作中集中处理，但需要一个KernFS将更新(包括数据和元数据)应用到文件系统中，这将导致再次进行数据复制。两个文件系统中的可信进程(如TFS或KernFS)也负责并发控制，这不可避免地成为高并发下的瓶颈。

在本文中，我们通过引入内核-用户空间协作体系结构(Kuco)来重新审视文件系统设计，以实现直接访问性能和高可伸缩性。Kuco遵循经典的客户端/服务器模型，包含两个组件，包括一个用户空间库(名为Ulib)，用于提供基本文件系统接口，以及一个位于内核中的可信线程(名为Kfs)，用于处理由Ulib发送的请求并执行关键更新(例如，元数据)。

受分布式文件系统设计的启发，例如AFS，它通过最小化服务器负载和减少客户端/服务器交互来提高可伸缩性，Kuco提出了一种新颖的任务划分和Ulib与Kfs之间的协作，它将大多数任务转移到Ulib，以避免Kfs可能出现的瓶颈。为了实现元数据的可伸缩性，我们引入了一种协作索引技术，允许Ulib在向Kfs发送请求之前执行路径名解析。通过这种方式，Kfs可以使用由Ulib提供的预先定位的地址直接更新元数据项。对于数据的可伸缩性，我们首先提出了一个两级锁机制来协调并发写入共享文件。具体来说，Kfs管理每个文件的写租约，并将其分配给打算打开该文件的进程。相反，这个进程中的线程在用户空间中使用范围锁来锁定文件。其次，我们引入了一个版本化的读协议来实现直接读，即使不与Kfs交互，尽管存在并发写入器。

Kuco还包括加强数据保护和提高基线性能的技术。Kuco以只读模式将PM空间映射到用户空间，以防止有bug的程序破坏文件数据。用户空间的直接写是通过一个三相写协议来实现的。在Ulib写入文件之前，Kfs通过切换页表中的权限位，将相关的PM页面从只读切换到可写。在编写文件时，还使用了一种预分配技术来减少Ulib和Kfs之间的交互。

利用Kuco架构，我们构建了一个名为KucoFS的PMfile系统，它获得了用户空间直接访问性能，同时提供了高可伸缩性。我们通过文件系统基准测试和实际应用来评估KucoFS。评估结果表明，在高竞争的工作负载下(例如，在同一个目录中创建文件或在一个共享文件中写入数据)，KucoFS比现有文件系统的扩展性好一个数量级，并且在低竞争的情况下提供略高的吞吐量。对于正常的数据操作，也会影响PM设备的带宽。综上所述，我们做出了以下贡献:

- 我们对最先进的PMawarefile系统进行了深入的分析，并总结了它们在解决软件开销和可伸缩性问题方面的局限性。
- 我们引入了Kuco，这是一种用户空间-内核协作体系结构，具有三种关键技术，包括协作索引、两级锁定和版本化读取，以实现高可伸缩性。
- 基于Kuco架构，我们实现了一个名为KucoFS的PMfile系统，实验表明，KucoFS在元数据操作方面达到了一个数量级以上的可伸缩性，并充分利用PM带宽进行数据操作。

# 2. 动机

在过去的十年中，研究人员已经开发了许多PMfile系统，如BPFS、SCMFS、PMFS、HiNFS、NOVA、Aerie、Strata、SplitFS、ZoFS。它们大致分为三类。首先,内核级文件系统。应用程序通过捕获到内核中进行数据和元数据操作来访问它们。第二，用户空间文件系统(例如，Aerie、Strata和ZoFS)。其中，Aerie依赖于可信进程(trusted process, TFS)来管理元数据，并保证元数据的完整性。TFS还通过分布式锁服务协调对共享文件的并发读写。相反，Strata允许应用程序直接将更新附加到每个进程的日志中，但需要后台线程(KernFS)将记录的数据异步地消化到存储设备中。ZoFS避免使用集中式组件，并允许用户空间应用程序在名为Intel Memory Protection Key (MPK)的新硬件特性的帮助下直接更新元数据。请注意，Aerie、Strata和ZoFS仍然依赖于内核来执行粗粒度的分配和保护。第三，混合文件系统(例如，SplitFS和我们提议的Kuco)。SplitFS提供了用户空间库和现有内核文件系统之间的粗粒度分割。它完全在用户空间中处理数据操作，并通过Ext4文件系统处理元数据操作。表1总结了现有的pm感知文件系统，以及它们在各个方面的表现。

![](/images/KucoFS/NVM-aware_file_systems.png)

![](/images/KucoFS/NOVA_overhead.png)

- **多核的可伸缩性**。NOVA是一种最先进的PM内核文件系统，经过精心设计，通过引入逐核分配器和逐inode日志来提高可伸缩性。然而，VFS仍然限制了它对某些操作的可伸缩性。我们通过在Intel Optane dcpmm上部署NOVA(详细的实验设置见§5.1)，并使用多个线程在同一个目录中创建、删除或重命名文件，实验证明了这一点。如图1a所示，随着线程数量的增加，它们的吞吐量几乎没有变化，因为VFS需要获取父目录的锁。Aerie依赖于一个集中的TFS来处理元数据操作和执行并发控制。虽然Aerie通过批量修改元数据来减少与TFS的通信，但§5中的评估显示，TFS仍然不可避免地成为高并发下的瓶颈。在Strata中，KernFS需要在后台消化日志数据和元数据。如果应用程序的日志完全用完，它必须等待正在进行的摘要完成，然后才能回收日志空间。因此，消化线程的数量限制了Strata的整体可伸缩性。Aerie和Strata都通过昂贵的ipc与可信进程(TFS/KernFS)交互，这将引入额外的系统调用开销。ZoFS不需要集中式组件，因此可以实现更高的可伸缩性。然而，当处理需要从内核分配新空间的操作时(例如，create和append，参见他们论文中的图7d、7f和7g)， ZoFS仍然不能很好地扩展。我们的评估表明，SplitFS对于数据和元数据操作的伸缩性都很差，因为它1)不支持不同进程之间的共享，2)依赖于Ext4更新元数据(见图7和图9)。
- **软件开销**。在内核中放置文件系统会面临两种类型的软件开销，即系统调用和VFS开销。我们仍然通过分析NOVA来研究这种开销，在NOVA中我们收集了常见文件系统操作的延迟分解。每个操作都用一个线程在100万个文件或目录上执行。我们从图1b中得到两个观察结果。首先，系统调用占用总执行时间的21%(例如stat和open)。另外，在一个进程进入内核后，操作系统可能会安排其他任务，然后将控制权返回给原来的进程。因此，系统调用为对延迟敏感的应用程序带来了额外的不确定性。其次，Linux内核文件系统是通过覆盖VFS函数来实现的，而VFS会导致不可忽视的开销。虽然最近的PM文件系统使用直接访问(DAX)绕过VFS中的页面缓存，但我们发现对于NOVA来说，仍有34%的时间花在VFS层上。ZoFS在用户空间部署文件系统，以避免陷入内核;然而，它仍然会带来额外的软件开销。ZoFS允许用户空间应用程序直接更新元数据，这可能导致正常程序在访问被恶意攻击者破坏的元数据时被终止。为了实现优雅的错误返回，ZoFS在每次系统调用开始时调用sigsetjump指令，这将导致额外的延迟(约200 ns)。SplitFS需要一个内核文件系统来处理元数据操作，所以它仍然会带来内核开销。
- **其他问题**。首先，使用不当的指针可能导致写入错误的位置并损坏数据，这被称为偏离写入。Strata向用户空间应用程序公开每个进程的操作日志和DRAM缓存(包括元数据和数据)。Aerie和SplitFS映射文件系统映像的子集到用户空间。因此，偶然的写操作很容易破坏这些区域的数据，而且这种破坏在NVM中是永久的，即使是在重新引导之后。其次，Aerie、Strata和SplitFS通过延迟新写入数据到其他进程的可见性来提高性能，直到发出fsync，迫使应用程序做出相应的调整。第三，ZoFS严重依赖于MPK机制，如果应用程序也需要使用MPK，它们可能会竞争有限的MPK资源。

总而言之，现有的文件系统设计很难实现高可伸缩性和低软件开销，这促使我们引入Kuco架构。

# 3. Kuco架构

在这篇文章中，我们介绍了Kuco架构来说明一个客户/服务器模型可以被采用来同时实现这两个目标。Kuco的核心思想是客户端和服务器之间的细粒度任务分工和协作，大部分负载都被转移到客户端，以避免服务器成为瓶颈。

## 3.1 总览

图2显示了Kuco体系结构。它遵循客户机/服务器模型，其中包括两个部分，包括一个用户空间库和一个全局内核线程，这两个部分分别称为Ulib和Kfs。应用程序通过链接Ulib来访问Kuco，不同的Ulib实例(即应用程序)通过单独的内存消息缓冲区与Kfs交互。像现有的用户空间系统，Kuco映射PM空间到用户空间，以支持直接读写访问。为了保护文件系统元数据不被破坏，Kuco不允许应用程序直接更新元数据;相反，这些请求被发布到Kfs，然后Kfs代表它们更新元数据。

![](/images/KucoFS/Kuco_architecture.png)

Kuco通过细粒度任务划分和Ulib与Kfs之间的协作提供了高可伸缩性。对于元数据的可伸缩性，Kuco整合了协作索引机制，将路径名遍历作业从Kfs卸载到用户空间(§3.2)。Ulib不会直接向Kfs发送元数据操作(如创建或解除链接)，而是在用户空间中查找所有相关的元数据项，然后在发送请求之前将这些信息封装在请求中。因此，Kfs可以直接对给定的地址执行元数据修改。为了数据的可伸缩性，使用了两级锁机制来处理并发写入共享文件(§3.3)。具体来说，Kfs使用基于租赁的分布式锁来解决不同应用程序(或进程)之间的写冲突。来自同一个进程的并发写操作使用一个纯用户空间范围锁进行序列化，可以在不需要Kfs的情况下获取该锁。Kuco进一步引入了版本化的读取技术来执行用户空间中的文件读取(§3.5)。通过在数据块映射(将逻辑文件数据映射到物理PM地址)中添加额外的版本位，Kuco可以读取一致版本的数据块，而不需要与Kfs交互来获取锁，尽管还有其他并发写入器。

为了进一步防止有bug的程序破坏文件数据，PM空间以只读模式映射到用户空间。Kuco通过在内核中使用三相写协议(§3.4)将Kfs放置在只读地址上，从而允许用户空间直接写。在Ulib写文件之前，Kfs首先修改页表中的权限位，将相关的数据页从只读切换到可写。为了进一步减少编写文件时Ulib和Kfs之间的交互次数，Kuco采用了预分配(pre-allocation)，这样Ulib可以从Kfs分配比预期更多的空闲页面。Kuco的PM空间除了有防止误写的写保护机制外，还被划分为不同的分区树，这些分区树作为读保护的最小单元。通过将Kuco应用于名为KucoFS的文件系统中，将所有技术结合在一起，获得了直接访问性能，提供了高可扩展性，并确保了内核级的数据保护。

## 3.2 协作索引

在典型的客户机-服务器模型中，每当Kfs收到元数据请求时，它都需要通过执行从根inode到包含该文件的目录的迭代路径名解析来查找相关的元数据(例如，描述文件属性的inode，或将文件名称映射到inode编号的dentries)。这种路径名遍历开销对Kfs来说是一个沉重的负担，特别是当目录包含大量子文件或具有很深的目录层次结构时。

为了解决这个问题，我们建议将路径名解析任务从Kfs转移到Ulib。通过将分区树映射到用户空间，Ulib可以直接在用户空间中找到相关的元数据项，然后通过在请求中封装元数据地址，向Kfs发送元数据更新请求。通过这种方式，Kfs可以直接用给定的地址更新元数据，并且路径名解析开销从Kfs转移到用户空间。

![](/images/KucoFS/create_file_with_collaborative_indexing.png)

图3显示了Kuco如何创建一个路径名为“/Bob/a”的文件。Ulib首先在“Bob”(➀)的dentry列表中找到前任dentry文件“a”。然后它向Kfs发送一个创建请求，并且消息中也包含了前任的地址(➁)。Kfs在收到请求(➂➃)之后创建这个文件，其中包括为这个文件创建一个inode，然后在父目录的dentry列表中插入一个新的dentry，该dentry具有给定的前身。要删除一个文件，这个文件的inode和父目录中的dentry都应该被删除，所以在Ulib发送它之前，它们的地址都保存在unlink请求中。请注意，atime在默认情况下是禁用的，启用了在用户空间中执行只读操作(如stat、readdir)，而不会向Kfs提交额外的请求。

在Kuco中，Ulibs生成指针，Kfs消耗它们。这种“单向”的指针共享范式简化了确保Kuco的正确性和安全性。一方面，元数据项被放置在一个有单独地址空间的元数据区域中，Ulib只能传递两种类型的元数据项的地址(即dentry和inode)。因此，我们在每个元数据项的开头添加一个标识符字段，这有助于Kfs检查元数据类型——任何不在元数据区域或没有指向dentry/inode的地址都被认为是无效的。另一方面，Kfs也基于文件系统内部逻辑执行一致性检查:

- 首先，Ulib可能读取不一致的目录树。例如，当Kfs在一个目录中创建新文件时，并发的Ulibs可能会读取该目录不一致的dentry列表。为了解决这个问题，我们用一个跳表来组织每个目录的dentry列表，每个dentry都由文件名的哈希值作为索引。跳表具有多层类似链表的数据结构。每个较高的层充当较低列表层的“快速通道”。基于列表的结构可以通过执行指针操作实现无锁的原子更新。此外，单个Kfs只对dentry列表执行插入和删除操作，包括重命名操作，重命名操作是通过先插入新节点，然后删除旧节点来执行的。因此，即使没有获取锁，对dentry的读也总是一致的。
- 其次，使用这种无锁设计，用户空间应用程序可能会读取Kfs正在删除的元数据项，从而导致“删后读”异常。为了安全回收已删除的项，我们需要确保不再有线程访问它。我们通过使用基于时代的回收机制来解决这个问题。EBR维护一个全局期和三个回收队列，其中执行被划分为多个时期，并在最后三个时期维护回收队列。每个线程还拥有一个私有的epoch。在epoch e中删除的项被放入到epoch e的队列中。每次Ulib启动操作时，它读取全局epoch，并更新自己的epoch，使其等于全局epoch。然后检查其他Ulib的私有epoch。如果所有的Ulibs在当前epoch e中都是活跃的，那么一个新的epoch开始了。此时，e或e+1中的所有线程都是活动的，与e-1相关的队列中的项目可以被安全回收。我们还在每个inode/dentry中添加了一个dirtyflag。Kfs通过将其dirtyflag设置为无效状态来删除元数据项，从而防止应用程序读取已经删除的项。
- 第三，Kfs需要正确处理冲突的元数据操作。例如，当多个Ulib同时执行元数据操作时，一个Ulib的预定位元数据项可能会在Kfs访问它之前被另一个并发的Ulib删除或重命名。因此，这个项目不再有效，它的地址不能再被Kfs使用。恶意进程也可能通过提供任意地址来攻击Kfs。幸运的是，只有Kfs可以更新元数据，并且它可以在处理操作之前验证预先定位的元数据。具体来说，Kfs检查预先定位的项是否仍然存在或仍然是前任，并避免创建同名的文件。当验证失败时，Kfs将解析路径名本身，并在操作失败时向Ulib返回一个错误代码。

**讨论**。首先，Kuco确保所有元数据操作都是原子处理的。对于create, Kfs只有在创建了一个inode之后才会在跳跃列表中插入一个新的dentry，以使被创建的文件可见;对于unlink，它会在删除其他字段之前自动删除dentry。重命名涉及更新两个dentry(在目标路径中创建一个新条目，然后删除旧条目)，因此程序可以在某个时间点在两个位置上看到两个相同的文件。我们在每个dentry中利用dirtyflag来防止这种不一致的状态。具体来说，源路径上的旧条目在创建新条目之前被设置为dirty，然后在创建新条目之后被设置为无效。作为一个整体，我们可以观察到元数据操作总是自动地改变目录树，即使不获取锁，Ulib也可以保证具有一致的目录树视图。其次，通过避免使用锁，Kuco的可伸缩性得到了进一步的提高——并发元数据更新都被委托给全局Kfs，因此它们可以在没有任何锁开销的情况下被处理(只有Kfs可以更新元数据)。Kuco通过操作日志确保元数据崩溃的一致性，这将在§4.2中讨论。

## 3.3 两级锁

Kuco引入了一个两级锁服务来协调对共享文件的并发写操作，这就避免了Kfs频繁地参与并发控制。首先，Kfs分配文件的写租期(在内核中，参见图2)来强制不同进程之间的粗粒度协调，类似Aerie和Strata。只有持有有效的写租约(尚未过期)的进程才能写该文件。我们假设Ulib很少申请租约，这是基于这样一个事实，即多个进程频繁地并发地写同一个文件并不常见。可以通过共享内存或管道实现进程间更细粒度的共享。在Kuco不需要读租(见章节3.5)。

![](/images/KucoFS/direct_access_range_lock.png)

其次，我们引入了一个直接访问范围锁来序列化同一进程中线程之间的并发写操作。一旦Ulib获得了文件的写租期，它就会在用户空间中为这个文件创建一个范围锁，这实际上是一个DRAM环缓冲区(如图4所示)。线程通过首先获取范围锁来写入文件，如果发生锁冲突，它就会被阻塞。环形缓冲区中的每个槽都有五个字段，分别是state、offset、size、ctime和checksum。校验和是前四个字段的哈希值。我们还在每个循环缓冲区的头部放置一个版本，以描述每个写操作的顺序。为了获得文件的锁，ulib首先使用原子的fetch_and_add来增加版本(①)。然后，它将一个锁项插入到环缓冲区的特定槽位中(②和③，这个位置由获取的版本号对环缓冲区大小取模确定)。当这个插槽与环形缓冲区的头部重叠时，插入被阻塞。在此之后，Ulib向后遍历循环缓冲区，以找到第一个冲突的锁项(即它们的写入数据重叠)。如果存在这样的冲突，Ulib会验证它的校验和，然后对它的状态进行轮询，直到它被释放(④)。如果线程在释放锁之前中止，Ulib还会反复检查它的ctime字段，以避免死锁。通过这种设计，多个线程可以在同一个文件中并发地写入不同的数据页。

## 3.4 三相写

一旦锁已经被获得，Ulib就可以实际写入文件数据。因为PM空间以只读模式映射到用户空间，所以Ulib不能直接写文件数据。相反，我们提出了一个三相写协议来执行直接写。为了确保崩溃的一致性，Kuco采用了copy-on-write (CoW)方法来写文件数据，其中新写入的数据总是重定向到新的PM页面。与NOVA和PMFS]类似，我们使用4KB作为默认的数据页面大小。Kuco中的写协议包括三个步骤。首先，Ulib通过两级锁定锁定文件，并向Kfs发送请求来分配新的PM页面。注意，通过使用CoW方式，覆盖和追加操作都需要空间分配。Kfs还需要修改相关的页表条目，以便在返回响应消息之前使这些分配的PM页面可写。其次，Ulib将未修改的数据从旧位置复制到用户缓冲区，并将新数据复制到已分配的PM页面，并通过flush指令将它们持久化。第三，Ulib向Kfs发送另一个请求来更新这个文件的元数据(即inode、块映射)，将新写的页面切换为只读，最后释放锁。

此外，我们引入了预分配机制，以避免为每个写操作从Kfs分配新的PM页。具体来说，我们允许Ulib从Kfs分配比预期更多的空闲页面(在我们的实现中，每次分配4 MB)。通过这种方式，Ulib可以使用本地空闲PM页面，而不需要与Kfs进行大多数写操作。当应用程序退出时，将未使用的页面返回给Kfs。对于异常退出，这些空闲页面暂时不能被其他应用程序重用，但仍然可以在恢复阶段回收(见§4.2)。预分配还有助于减少更新页表项的开销。当Kfs在每次分配后更新页表项时，它需要显式刷新相关的TLB项，以使修改可见。预分配允许一次分配多个数据页，因此TLB条目可以批量刷新。

## 3.5 版本化读取

在写协议中，由于CoW方式，新旧版本的数据页都被临时保留，这为我们提供了在不阻塞写操作的情况下读取数据的机会。但是，将逻辑文件映射到物理页面的块映射仍然由Kfs及时更新。这促使我们设计版本化的读机制来实现用户级的直接读，而不需要任何Kfs的参与，不管并发的写程序是什么。

![](/images/KucoFS/versioned_read_protocal.png)

版本化读取的设计目的是允许用户空间读取而不锁定文件，同时确保读取者不会从不完整的写入中读取数据。为了实现这一点，Kuco使用了一个类似于ext2的块映射来索引数据页，并在块映射的每个指针中嵌入一个版本字段。如图5所示，每个96位块映射项包含四个字段，分别是start、version、end和pointer。对于写操作，例如写3个数据页，Kfs按照如下格式更新对应的块映射项:1\|V1\|0\|P1，0\|V1\|0\|P2，0\|V1\|1\|P3。特别是，这三个项目共享同一个版本(即V1)，这是由Ulib在获取范围锁时提供的(在3.3节中)。第一项的起始位和最后一项的结束位设置为1。我们只为指针字段保留40位，因为它指向一个4kb对齐的页面，较低的12位可以丢弃。

在这种格式下，当遇到图5中的三种情况之一时，读取器可以读取一致的数据页快照:

- a) 没有重叠。当在不重叠的页面上对文件执行两次更新时，具有相同版本的项目应该同时包含一个开始位和一个结束位(在情况a中是V1和V2)。
- b) 重叠结束部分。当线程重写前一个写的结束部分时，当版本增加时，读取器应该总是看到一个开始位(V1➔V3)。
- c) 与前面部分重叠。当一个线程重写前一个写的前半部分时，读取器应该总是在版本下降之前看到一个结束位(V4➔V3)。

如果Ulib满足上述三种情况之外的任何情况，则表明Kfs正在更新一些其他未完成写操作的块映射。在这种情况下，Ulib需要通过重新扫描相关版本的序列再次验证。在Ulib成功进行版本检查后，它然后读取相关的数据页。总的来说，Kuco利用嵌入式版本来检测不完整的写操作和重试操作，直到读取一致的数据快照。

**读语义**。在多线程/进程执行中，版本化读与传统锁定读略有不同，因为它允许并发写。例如，一个写操作已经开始，但还没有完成，但是在这中间，有一个读操作，读取数据的一个旧快照。在本例中，执行仍然等于一个可序列化的顺序(例如，“read➛write”，“➛”表示happens-before)。在每个线程中，版本化读与锁定读具有相同的语义，因为一个读或写必须在发出下一个读或写之前完成。

# 4. KucoFS实现

在本节中，我们将描述如何在名为KucoFS的持久内存文件系统中应用Kuco架构。

## 4.1 数据布局

KucoFS使用DRAM和PM的混合方式来组织Kuco的分区树(图6)。在DRAM中，一个指针数组(inode表)被放置在一个预定义的位置，以指向实际的inode。inode表中的第一个元素指向当前分区树的根inode。这样，Ulib就可以从用户空间的根节点查找任何文件。如前所述，目录的dentry列表被组织成一个跳跃列表，它也被放置在DRAM中。

![](/images/KucoFS/partition_tree.png)

为了提高效率，KucoFS仅在正常请求时对DRAM元数据进行操作。为了确保元数据的持久性和崩溃一致性，KucoFS在PM中为每个分区树放置了一个仅追加的持久操作日志。当Kfs更新元数据时，它首先自动附加一个日志条目，然后实际更新DRAM元数据(见§4.2)。当系统发生故障时，可以通过重放操作日志中的日志项来恢复DRAM元数据。除了操作日志之外，额外的PM空间被分割成4KB的数据页和元数据页。空闲PM页面使用PM中的位图和DRAM中的空闲列表来管理(用于快速分配)，并且位图在检查点阶段由Kfs延迟持久化。

## 4.2 崩溃一致性及恢复

**元数据的一致性**。KucoFS通过命令更新DRAM和PM来确保元数据的一致性。图6显示了Kfs在从Ulib接收到创建请求时如何创建文件的步骤。在➊处，Kfs从inode表中保留了一个未使用的inode号，并在操作日志中添加了一个日志项。这个日志条目记录了inode号、文件名、父目录inode号和其他属性。在➋中，它分配一个每个字段都被填充的inode，并更新inode表以指向这个inode。在➌中，它然后将一个dentry插入到dentry列表中，该dentry具有前任的给定地址，以使被创建的文件可见。如果同一个dentry已经存在，则创建失败(避免创建相同的文件)。要删除一个文件，Kfs首先添加一个日志条目，删除父目录中具有给定地址的dentry，最后释放相关空间(如inode、数据页和块映射)。如果在操作完成之前发生了崩溃，那么DRAM元数据更新将丢失，但是Kfs可以在恢复之后通过重放日志来将它们重构到最新的状态。对于重命名操作，除了系统故障外，内核线程可能会崩溃，并导致dirtyflag处于不一致的状态。但是，如果内核线程崩溃，我们会考虑整个文件系统崩溃，这需要重新引导文件系统，并且上面的日志技术确保重命名操作也与崩溃保持一致。

**数据一致性**。KucoFS处理文件的写操作时，首先以CoW的方式更新数据页面，然后在操作日志中附加一个日志条目，记录元数据的修改。此时，写操作被认为是持久的。然后，KucoFS可以安全地更新DRAM元数据，使此操作可见。当在日志条目被持久化之前发生系统故障时，KucoFS可以回滚到它的最后一个一致状态，因为旧的数据和元数据不会被修改。否则，这个写操作在恢复后通过重放操作日志可见。

**日志清理和恢复**。我们引入了检查点机制，以避免操作日志任意增长。当Kfs不繁忙或日志大小超过阈值(在我们的实现中为1MB)时，我们使用一个后台内核线程来触发一个检查点，该检查点将操作日志中的元数据修改应用到PM元数据页面。用于管理PM空闲页面的位图也被更新和持久化。之后，操作日志被截断。后台消化不会阻塞前端操作，唯一的影响是日志清理会消耗额外的PM带宽。然而，元数据通常规模较小，带宽消耗也不高。

每次KucoFS从崩溃中重新启动时，Kfs首先会在操作日志中重放未检查的日志条目，以便使PM元数据页面更新。然后它将PM元数据页复制到DRAM。根据存储在PM中的位图重新构建PM数据页的空闲列表。不需要担心在恢复期间再次崩溃，因为日志还没有被截断，可以再次播放。在DRAM和PM之间保持元数据的冗余副本会增加PM/DRAM空间的消耗，但我们认为这是值得的。有了DRAM中的结构化元数据，我们可以直接在DRAM中执行快速索引;在日志中追加日志条目可以节省更新pm的次数，从而减少了持久性开销。今后，我们计划通过只在DRAM中保留活动元数据来减少DRAM的占用空间。

## 4.3 写保护

KucoFS严格控制对文件系统镜像的更新。内存中的元数据和持久操作日志都是关键的，因此内核中的Kfs是唯一允许更新它们的。文件页以只读模式映射到用户空间。应用程序只能将数据写入新分配的PM页面，而不能修改现有的数据页面。KucoFS还为用户空间数据结构提供进程级隔离。消息缓冲区和范围锁是每个进程私有的，因此攻击者无法在其他进程中访问它们，除非它执行特权升级攻击。这些安全问题超出了本工作的范围。因此，我们认为KucoFS实现了与内核文件系统相同的写保护。

防止偏离写。与许多现有的用户空间文件系统不同，一些PM文件系统容易受到偏离写的攻击，KucoFS通过将PM空间映射为只读模式来防止这个问题。请注意，在写操作完成后，在权限位改变之前，仍然有一个临时的可写窗口(小于1µs)用于新写的页面。这是不可避免的，就像在现有的内核文件系统(如PMFS)中一样。幸运的是，这种情况很少发生。此外，用户空间中的范围锁和消息缓冲区也可能被偏离写操作破坏。对于这个威胁，我们在每个槽上添加checksum和租约字段，它们可用于检查插入的元素是否被破坏。

## 4.4 读保护

KucoFS用分区树组织目录树，分区树充当访问控制的最小单元。每个分区树都是自包含的，包括PM中的元数据和数据，以及DRAM中的相关元数据拷贝。KucoFS不允许文件/目录结构跨不同的分区。当一个程序访问KucoFS时，只有它可以访问的分区树被映射到它的地址空间，但是其他的分区树对它是不可见的。

在KucoFS中，通过以下妥协加强了读访问控制。首先，类似于现有的用户空间文件系统，KucoFS不支持“只写”或复杂的权限语义，如POSIX访问控制列表(acl)，因为现有的页表只有一个位来表示页面是只读或读写的。其次，KucoFS不支持用户之间灵活的数据共享，因为使用分区树设计很难改变特定文件的权限(例如，通过chmod)。但是有几种实用的方法:① 创建一个独立的分区，让具有不同权限的应用程序可以访问它;② 在不同应用之间下发用户级rpc获取数据。我们认为这种权衡不太可能成为障碍，因为KucoFS仍然支持同一个用户内的应用程序之间的高效数据共享，这是现实世界中场景中更常见的情况。

## 4.5 内存映射I/O

在“写时复制”系统中支持DAX特性需要额外的努力，因为在正常的写操作中更新文件的位置不对。此外，DAX为程序员正确使用具有原子性和崩溃一致性的PM空间带来了巨大的挑战。考虑到这些因素，我们借用了NOVA的想法来提供原子mmap，它有更高的一致性保证。当应用程序将文件映射到用户空间时，Ulib将文件数据复制到其私有管理的数据页面，然后向Kfs发送请求，将这些页面映射到连续地址空间。当应用程序发出一个msync系统调用时，Ulib就会把它作为一个写操作来处理，这样就可以原子地使这些数据页面中的更新对其他应用程序可见。

## 4.6 KucoFS的API

KucoFS提供了一个类posix的接口，因此现有的应用程序可以在不修改源代码的情况下访问它。我们通过设置LD_PRELOAD环境变量来实现这一点。Ulib拦截标准C库中所有与文件系统操作相关的api。如果访问文件的前缀与预定义的字符串(例如/kuco/usr1)匹配，则Ulib直接处理系统调用。否则，系统调用将以旧模式处理。注意，读或写操作只传递在参数列表中的文件描述符。Ulib通过一个映射表将它们与旧的系统调用区分开来，该映射表跟踪KucoFS的文件。

## 4.7 例子:将它们全部放在一起

最后，我们通过一个将4kb的数据写入一个新文件，然后将其读取出来的例子来总结Kuco架构和KucoFS的设计。

**Open**。在发送打开文件的请求之前，Ulib首先预先定位相关的元数据。因为这是一个新文件，所以Ulib不能直接找到它。相反，它会在父目录的dentry列表中找到前身，以便以后创建。地址，以及其他信息(例如，文件名，O_CREAT flags，等等)都封装在开放请求中。当Kfs接收到请求时，它会根据给定的地址创建这个文件。它还需要为这个进程分配写租约。然后，Kfs发送一条响应消息。在此之后，Ulib为这个打开的文件创建一个文件描述符和一个范围锁，并返回到应用程序。

**Write**。然后，应用程序通过Ulib使用写调用将4KB的数据写入这个新创建的文件。首先，Ulib试图通过两阶段锁定服务锁定文件。因为写租约仍然有效，所以它直接通过范围锁来获取锁。当出现写冲突时，Ulib阻塞程序，并等待其他并发线程释放锁。在此之后，Ulib可以成功地获取锁。然后，它从预分配的页面中分配一个4 kb的页面，将数据复制到其中，并将它们从CPU缓存中刷新出来。一旦预分配的空间用完，Ulib还需要向Kfs发出一个额外的请求来分配更多的空闲数据页。最后，Ulib将写请求发送给Kfs以完成其余步骤，包括将已写数据页的权限位更改为只读，附加一个日志条目来描述此写操作，以及更新DRAM元数据。Ulib最后解锁范围锁中的文件。

**Read**。KucoFS可以在不与Kfs交互的情况下读取文件数据。要从这个文件中读取前4KB, Ulib寻找用户空间中的inode并读取第一个块映射项。执行版本检查以确保其状态满足3.5节中描述的三个条件之一。在此之后，Ulib可以安全地读取映射项中的指针所指向的数据页。

**Close**。在关闭这个文件时，Ulib还需要向Kfs发送一个关闭请求。Kfs然后收回写租约，因为它不再访问这个文件了。

# 5. 实验评估

在我们的评估中，我们试图回答以下问题:

- KucoFS是否实现了提供直接访问性能和高可伸缩性的目标?
- KucoFS中的每一项技术是如何帮助实现上述目标的?
- KucoFS在宏观基准测试和实际应用中表现如何?

## 5.1 实验设置



# 6. 相关工作

**内核-用户空间协作**。将I/O操作从内核移动到用户空间的想法已经得到了很好的研究。Belay等人的Dune利用现代处理器中的虚拟化硬件抽象出Dune进程。它允许直接访问用户空间中的特权CPU指令，并以减少开销的方式执行系统调用。在Dune的基础上，IX进一步提高了数据中心应用程序的性能，将内核(控制平面)的管理和调度功能与网络处理(数据平面)分离开来。Arrakis是一种新的网络服务器操作系统，应用程序可以直接访问I/O设备，内核只执行粗粒度保护。FLEX通过将传统的文件操作替换为类似的基于DAX的操作来避免内核开销，这些操作与SplitFS有一些相似之处。虽然这些系统共享在内核和用户空间之间划分任务的相同想法，但KucoFS的不同之处在于，它在执行密切协作的同时展示了细粒度的职责划分。

**持久内存存储系统**。除了前面提到的持久化内存文件系统，我们在这里总结了更多的PM系统。首先，一般的PM优化。Yang等人从微观和宏观层面探讨了Optane DCPMM的性能特性和特点，并提供了一系列的指导方针，以实现性能的最大化。Libnvmmio通过崩溃原子性扩展了用户空间内存映射I/O。最近的许多论文也设计了各种数据结构，它们在持久内存中正确而有效地工作。第二，PM感知的文件系统。BPFS采用短路影子分页，保证元数据和数据的一致性。SCMFS通过在现有操作系统中使用虚拟内存管理(VMM)将文件映射到相邻的虚拟地址区域，从而简化了文件管理。NOVA-Fortis通过提供快照机制进一步提高了容错能力。Ziggurat是一个分级文件系统，用于估计文件数据的冷热，并将冷数据从PM迁移到磁盘。DevFS将文件系统实现推送到具有计算能力和设备级RAM的存储设备中。第三，分布式PM系统。Hotpot使用分布式共享持久内存架构管理集群中不同节点的PM设备。Octopus通过减少软件开销，利用PM和RDMA来构建一个高效的分布式文件系统。类似地，Orion也是分布式持久内存文件系统，但构建在内核中。FlatStore是一个基于RDMA网络的日志结构键值存储引擎;它通过批量处理小请求最小化刷新开销。

# 7. 总结

在本文中，我们引入了一个名为Kuco的内核和用户级协同架构，它在用户空间和内核之间展示了细粒度的任务划分。在Kuco的基础上，我们进一步设计并实现了一个名为KucoFS的PMfile系统，实验表明，KucoFS提供了高效和高可伸缩性的性能。