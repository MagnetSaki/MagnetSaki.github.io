---
layout: post
title: Scalable Persistent Memory File System with Kernel-Userspace Collaboration
date: 2022-01-11
category: translation
---

# 摘要

我们介绍了Kuco，一种新颖的直接访问文件系统架构，其主要目标是可伸缩性。Kuco利用三种关键技术——协作索引、两级锁定和版本控制读取——将耗时的任务(如路径名解析和并发控制)从内核转移到用户空间，从而避免了内核处理瓶颈。在Kuco的基础上，我们提出了KucoFS的设计和实现，并通过实验证明了KucoFS在广泛的实验范围内具有良好的性能;重要的是，在元数据操作方面，KucoFS比现有的文件系统有更好的扩展性，可以达到一个数量级，并且可以充分利用设备带宽进行数据操作。

# 1. 引言

新兴的可字节寻址持久存储器(PM)，如PCM、ReRAM和最近发布的Intel Optane DCPMM，提供接近DRAM的性能和类似于磁盘的数据持久性。这种高性能硬件增加了重新设计高效文件系统的重要性。在过去的十年里，系统社区已经提出了许多文件系统，如BPFS、PMFS和NOVA，以最小化由传统文件系统架构引起的软件开销。但是，这些pm感知的文件系统是操作系统的一部分，应用程序需要进入内核才能访问它们，其中系统调用和虚拟文件系统(VFS)仍然会产生不可忽略的开销。在这方面，最近的研究提出在用户空间中部署文件系统直接访问文件数据(即直接访问)，从而利用PM的高性能。

尽管做出了这些努力，我们发现另一个重要的性能指标——可伸缩性——仍然没有得到很好的解决，特别是当多核处理器遇上快速PM时。NOVA通过对内部数据结构进行分区和避免使用全局锁，提高了多核可伸缩性。但是，我们的评估表明，由于VFS层的存在，它仍然不能很好地伸缩。更糟糕的是，一些用户空间文件系统设计通过引入集中式组件进一步加剧了可伸缩性问题。例如，Aerie通过向具有更新元数据权限的可信进程(TFS)发送昂贵的进程间通信(ipc)来确保文件系统元数据的完整性。另一个例子是Strata，它通过直接在PM日志中记录更新，避免了在正常操作中集中处理，但需要一个KernFS将更新(包括数据和元数据)应用到文件系统中，这将导致再次进行数据复制。两个文件系统中的可信进程(如TFS或KernFS)也负责并发控制，这不可避免地成为高并发下的瓶颈。

在本文中，我们通过引入内核-用户空间协作体系结构(Kuco)来重新审视文件系统设计，以实现直接访问性能和高可伸缩性。Kuco遵循经典的客户端/服务器模型，包含两个组件，包括一个用户空间库(名为Ulib)，用于提供基本文件系统接口，以及一个位于内核中的可信线程(名为Kfs)，用于处理由Ulib发送的请求并执行关键更新(例如，元数据)。

受分布式文件系统设计的启发，例如AFS，它通过最小化服务器负载和减少客户端/服务器交互来提高可伸缩性，Kuco提出了一种新颖的任务划分和Ulib与Kfs之间的协作，它将大多数任务转移到Ulib，以避免Kfs可能出现的瓶颈。为了实现元数据的可伸缩性，我们引入了一种协作索引技术，允许Ulib在向Kfs发送请求之前执行路径名解析。通过这种方式，Kfs可以使用由Ulib提供的预先定位的地址直接更新元数据项。对于数据的可伸缩性，我们首先提出了一个两级锁机制来协调并发写入共享文件。具体来说，Kfs管理每个文件的写租约，并将其分配给打算打开该文件的进程。相反，这个进程中的线程在用户空间中使用范围锁来锁定文件。其次，我们引入了一个版本化的读协议来实现直接读，即使不与Kfs交互，尽管存在并发写入器。

Kuco还包括加强数据保护和提高基线性能的技术。Kuco以只读模式将PM空间映射到用户空间，以防止有bug的程序破坏文件数据。用户空间的直接写是通过一个三相写协议来实现的。在Ulib写入文件之前，Kfs通过切换页表中的权限位，将相关的PM页面从只读切换到可写。在编写文件时，还使用了一种预分配技术来减少Ulib和Kfs之间的交互。

利用Kuco架构，我们构建了一个名为KucoFS的PMfile系统，它获得了用户空间直接访问性能，同时提供了高可伸缩性。我们通过文件系统基准测试和实际应用来评估KucoFS。评估结果表明，在高竞争的工作负载下(例如，在同一个目录中创建文件或在一个共享文件中写入数据)，KucoFS比现有文件系统的扩展性好一个数量级，并且在低竞争的情况下提供略高的吞吐量。对于正常的数据操作，也会影响PM设备的带宽。综上所述，我们做出了以下贡献:

- 我们对最先进的PMawarefile系统进行了深入的分析，并总结了它们在解决软件开销和可伸缩性问题方面的局限性。
- 我们引入了Kuco，这是一种用户空间-内核协作体系结构，具有三种关键技术，包括协作索引、两级锁定和版本化读取，以实现高可伸缩性。
- 基于Kuco架构，我们实现了一个名为KucoFS的PMfile系统，实验表明，KucoFS在元数据操作方面达到了一个数量级以上的可伸缩性，并充分利用PM带宽进行数据操作。

# 2. 动机

在过去的十年中，研究人员已经开发了许多PMfile系统，如BPFS、SCMFS、PMFS、HiNFS、NOVA、Aerie、Strata、SplitFS、ZoFS。它们大致分为三类。首先,内核级文件系统。应用程序通过捕获到内核中进行数据和元数据操作来访问它们。第二，用户空间文件系统(例如，Aerie、Strata和ZoFS)。其中，Aerie依赖于可信进程(trusted process, TFS)来管理元数据，并保证元数据的完整性。TFS还通过分布式锁服务协调对共享文件的并发读写。相反，Strata允许应用程序直接将更新附加到每个进程的日志中，但需要后台线程(KernFS)将记录的数据异步地消化到存储设备中。ZoFS避免使用集中式组件，并允许用户空间应用程序在名为Intel Memory Protection Key (MPK)的新硬件特性的帮助下直接更新元数据。请注意，Aerie、Strata和ZoFS仍然依赖于内核来执行粗粒度的分配和保护。第三，混合文件系统(例如，SplitFS和我们提议的Kuco)。SplitFS提供了用户空间库和现有内核文件系统之间的粗粒度分割。它完全在用户空间中处理数据操作，并通过Ext4文件系统处理元数据操作。表1总结了现有的pm感知文件系统，以及它们在各个方面的表现。

![](/images/KucoFS/NVM-aware_file_systems.png)

![](/images/KucoFS/NOVA_overhead.png)

- **多核的可伸缩性**。NOVA是一种最先进的PM内核文件系统，经过精心设计，通过引入逐核分配器和逐inode日志来提高可伸缩性。然而，VFS仍然限制了它对某些操作的可伸缩性。我们通过在Intel Optane dcpmm上部署NOVA(详细的实验设置见§5.1)，并使用多个线程在同一个目录中创建、删除或重命名文件，实验证明了这一点。如图1a所示，随着线程数量的增加，它们的吞吐量几乎没有变化，因为VFS需要获取父目录的锁。Aerie依赖于一个集中的TFS来处理元数据操作和执行并发控制。虽然Aerie通过批量修改元数据来减少与TFS的通信，但§5中的评估显示，TFS仍然不可避免地成为高并发下的瓶颈。在Strata中，KernFS需要在后台消化日志数据和元数据。如果应用程序的日志完全用完，它必须等待正在进行的摘要完成，然后才能回收日志空间。因此，消化线程的数量限制了Strata的整体可伸缩性。Aerie和Strata都通过昂贵的ipc与可信进程(TFS/KernFS)交互，这将引入额外的系统调用开销。ZoFS不需要集中式组件，因此可以实现更高的可伸缩性。然而，当处理需要从内核分配新空间的操作时(例如，create和append，参见他们论文中的图7d、7f和7g)， ZoFS仍然不能很好地扩展。我们的评估表明，SplitFS对于数据和元数据操作的伸缩性都很差，因为它1)不支持不同进程之间的共享，2)依赖于Ext4更新元数据(见图7和图9)。
- **软件开销**。在内核中放置文件系统会面临两种类型的软件开销，即系统调用和VFS开销。我们仍然通过分析NOVA来研究这种开销，在NOVA中我们收集了常见文件系统操作的延迟分解。每个操作都用一个线程在100万个文件或目录上执行。我们从图1b中得到两个观察结果。首先，系统调用占用总执行时间的21%(例如stat和open)。另外，在一个进程进入内核后，操作系统可能会安排其他任务，然后将控制权返回给原来的进程。因此，系统调用为对延迟敏感的应用程序带来了额外的不确定性。其次，Linux内核文件系统是通过覆盖VFS函数来实现的，而VFS会导致不可忽视的开销。虽然最近的PM文件系统使用直接访问(DAX)绕过VFS中的页面缓存，但我们发现对于NOVA来说，仍有34%的时间花在VFS层上。ZoFS在用户空间部署文件系统，以避免陷入内核;然而，它仍然会带来额外的软件开销。ZoFS允许用户空间应用程序直接更新元数据，这可能导致正常程序在访问被恶意攻击者破坏的元数据时被终止。为了实现优雅的错误返回，ZoFS在每次系统调用开始时调用sigsetjump指令，这将导致额外的延迟(约200 ns)。SplitFS需要一个内核文件系统来处理元数据操作，所以它仍然会带来内核开销。
- **其他问题**。首先，使用不当的指针可能导致写入错误的位置并损坏数据，这被称为偏离写入。Strata向用户空间应用程序公开每个进程的操作日志和DRAM缓存(包括元数据和数据)。Aerie和SplitFS映射文件系统映像的子集到用户空间。因此，偶然的写操作很容易破坏这些区域的数据，而且这种破坏在NVM中是永久的，即使是在重新引导之后。其次，Aerie、Strata和SplitFS通过延迟新写入数据到其他进程的可见性来提高性能，直到发出fsync，迫使应用程序做出相应的调整。第三，ZoFS严重依赖于MPK机制，如果应用程序也需要使用MPK，它们可能会竞争有限的MPK资源。

总而言之，现有的文件系统设计很难实现高可伸缩性和低软件开销，这促使我们引入Kuco架构。

# 3. Kuco架构

在这篇文章中，我们介绍了Kuco架构来说明一个客户/服务器模型可以被采用来同时实现这两个目标。Kuco的核心思想是客户端和服务器之间的细粒度任务分工和协作，大部分负载都被转移到客户端，以避免服务器成为瓶颈。

## 3.1 总览

图2显示了Kuco体系结构。它遵循客户机/服务器模型，其中包括两个部分，包括一个用户空间库和一个全局内核线程，这两个部分分别称为Ulib和Kfs。应用程序通过链接Ulib来访问Kuco，不同的Ulib实例(即应用程序)通过单独的内存消息缓冲区与Kfs交互。像现有的用户空间系统，Kuco映射PM空间到用户空间，以支持直接读写访问。为了保护文件系统元数据不被破坏，Kuco不允许应用程序直接更新元数据;相反，这些请求被发布到Kfs，然后Kfs代表它们更新元数据。

![](/images/KucoFS/Kuco_architecture.png)

Kuco通过细粒度任务划分和Ulib与Kfs之间的协作提供了高可伸缩性。对于元数据的可伸缩性，Kuco整合了协作索引机制，将路径名遍历作业从Kfs卸载到用户空间(§3.2)。Ulib不会直接向Kfs发送元数据操作(如创建或解除链接)，而是在用户空间中查找所有相关的元数据项，然后在发送请求之前将这些信息封装在请求中。因此，Kfs可以直接对给定的地址执行元数据修改。为了数据的可伸缩性，使用了两级锁机制来处理并发写入共享文件(§3.3)。具体来说，Kfs使用基于租赁的分布式锁来解决不同应用程序(或进程)之间的写冲突。来自同一个进程的并发写操作使用一个纯用户空间范围锁进行序列化，可以在不需要Kfs的情况下获取该锁。Kuco进一步引入了版本化的读取技术来执行用户空间中的文件读取(§3.5)。通过在数据块映射(将逻辑文件数据映射到物理PM地址)中添加额外的版本位，Kuco可以读取一致版本的数据块，而不需要与Kfs交互来获取锁，尽管还有其他并发写入器。

为了进一步防止有bug的程序破坏文件数据，PM空间以只读模式映射到用户空间。Kuco通过在内核中使用三相写协议(§3.4)将Kfs放置在只读地址上，从而允许用户空间直接写。在Ulib写文件之前，Kfs首先修改页表中的权限位，将相关的数据页从只读切换到可写。为了进一步减少编写文件时Ulib和Kfs之间的交互次数，Kuco采用了预分配(pre-allocation)，这样Ulib可以从Kfs分配比预期更多的空闲页面。Kuco的PM空间除了有防止误写的写保护机制外，还被划分为不同的分区树，这些分区树作为读保护的最小单元。通过将Kuco应用于名为KucoFS的文件系统中，将所有技术结合在一起，获得了直接访问性能，提供了高可扩展性，并确保了内核级的数据保护。

## 3.2 协作索引

在典型的客户机-服务器模型中，每当Kfs收到元数据请求时，它都需要通过执行从根inode到包含该文件的目录的迭代路径名解析来查找相关的元数据(例如，描述文件属性的inode，或将文件名称映射到inode编号的dentries)。这种路径名遍历开销对Kfs来说是一个沉重的负担，特别是当目录包含大量子文件或具有很深的目录层次结构时。

为了解决这个问题，我们建议将路径名解析任务从Kfs转移到Ulib。通过将分区树映射到用户空间，Ulib可以直接在用户空间中找到相关的元数据项，然后通过在请求中封装元数据地址，向Kfs发送元数据更新请求。通过这种方式，Kfs可以直接用给定的地址更新元数据，并且路径名解析开销从Kfs转移到用户空间。

![](/images/KucoFS/create_file_with_collaborative_indexing.png)

图3显示了Kuco如何创建一个路径名为“/Bob/a”的文件。Ulib首先在“Bob”(➀)的dentry列表中找到前任dentry文件“a”。然后它向Kfs发送一个创建请求，并且消息中也包含了前任的地址(➁)。Kfs在收到请求(➂➃)之后创建这个文件，其中包括为这个文件创建一个inode，然后在父目录的dentry列表中插入一个新的dentry，该dentry具有给定的前身。要删除一个文件，这个文件的inode和父目录中的dentry都应该被删除，所以在Ulib发送它之前，它们的地址都保存在unlink请求中。请注意，atime在默认情况下是禁用的，启用了在用户空间中执行只读操作(如stat、readdir)，而不会向Kfs提交额外的请求。

在Kuco中，Ulibs生成指针，Kfs消耗它们。这种“单向”的指针共享范式简化了确保Kuco的正确性和安全性。一方面，元数据项被放置在一个有单独地址空间的元数据区域中，Ulib只能传递两种类型的元数据项的地址(即dentry和inode)。因此，我们在每个元数据项的开头添加一个标识符字段，这有助于Kfs检查元数据类型——任何不在元数据区域或没有指向dentry/inode的地址都被认为是无效的。另一方面，Kfs也基于文件系统内部逻辑执行一致性检查:

- 首先，Ulib可能读取不一致的目录树。例如，当Kfs在一个目录中创建新文件时，并发的Ulibs可能会读取该目录不一致的dentry列表。为了解决这个问题，我们用一个跳表来组织每个目录的dentry列表，每个dentry都由文件名的哈希值作为索引。跳表具有多层类似链表的数据结构。每个较高的层充当较低列表层的“快速通道”。基于列表的结构可以通过执行指针操作实现无锁的原子更新。此外，单个Kfs只对dentry列表执行插入和删除操作，包括重命名操作，重命名操作是通过先插入新节点，然后删除旧节点来执行的。因此，即使没有获取锁，对dentry的读也总是一致的。
- 其次，使用这种无锁设计，用户空间应用程序可能会读取Kfs正在删除的元数据项，从而导致“删后读”异常。为了安全回收已删除的项，我们需要确保不再有线程访问它。我们通过使用基于时代的回收机制来解决这个问题。EBR维护一个全局期和三个回收队列，其中执行被划分为多个时期，并在最后三个时期维护回收队列。每个线程还拥有一个私有的epoch。在epoch e中删除的项被放入到epoch e的队列中。每次Ulib启动操作时，它读取全局epoch，并更新自己的epoch，使其等于全局epoch。然后检查其他Ulib的私有epoch。如果所有的Ulibs在当前epoch e中都是活跃的，那么一个新的epoch开始了。此时，e或e+1中的所有线程都是活动的，与e-1相关的队列中的项目可以被安全回收。我们还在每个inode/dentry中添加了一个dirtyflag。Kfs通过将其dirtyflag设置为无效状态来删除元数据项，从而防止应用程序读取已经删除的项。
- 第三，Kfs需要正确处理冲突的元数据操作。例如，当多个Ulib同时执行元数据操作时，一个Ulib的预定位元数据项可能会在Kfs访问它之前被另一个并发的Ulib删除或重命名。因此，这个项目不再有效，它的地址不能再被Kfs使用。恶意进程也可能通过提供任意地址来攻击Kfs。幸运的是，只有Kfs可以更新元数据，并且它可以在处理操作之前验证预先定位的元数据。具体来说，Kfs检查预先定位的项是否仍然存在或仍然是前任，并避免创建同名的文件。当验证失败时，Kfs将解析路径名本身，并在操作失败时向Ulib返回一个错误代码。

**讨论**。首先，Kuco确保所有元数据操作都是原子处理的。对于create, Kfs只有在创建了一个inode之后才会在跳跃列表中插入一个新的dentry，以使被创建的文件可见;对于unlink，它会在删除其他字段之前自动删除dentry。重命名涉及更新两个dentry(在目标路径中创建一个新条目，然后删除旧条目)，因此程序可以在某个时间点在两个位置上看到两个相同的文件。我们在每个dentry中利用dirtyflag来防止这种不一致的状态。具体来说，源路径上的旧条目在创建新条目之前被设置为dirty，然后在创建新条目之后被设置为无效。作为一个整体，我们可以观察到元数据操作总是自动地改变目录树，即使不获取锁，Ulib也可以保证具有一致的目录树视图。其次，通过避免使用锁，Kuco的可伸缩性得到了进一步的提高——并发元数据更新都被委托给全局Kfs，因此它们可以在没有任何锁开销的情况下被处理(只有Kfs可以更新元数据)。Kuco通过操作日志确保元数据崩溃的一致性，这将在§4.2中讨论。

## 3.3 两级锁

Kuco引入了一个两级锁服务来协调对共享文件的并发写操作，这就避免了Kfs频繁地参与并发控制。首先，Kfs分配文件的写租期(在内核中，参见图2)来强制不同进程之间的粗粒度协调，类似Aerie和Strata。只有持有有效的写租约(尚未过期)的进程才能写该文件。我们假设Ulib很少申请租约，这是基于这样一个事实，即多个进程频繁地并发地写同一个文件并不常见。可以通过共享内存或管道实现进程间更细粒度的共享。在Kuco不需要读租(见章节3.5)。

![](/images/KucoFS/direct_access_range_lock.png)

其次，我们引入了一个直接访问范围锁来序列化同一进程中线程之间的并发写操作。一旦Ulib获得了文件的写租期，它就会在用户空间中为这个文件创建一个范围锁，这实际上是一个DRAM环缓冲区(如图4所示)。线程通过首先获取范围锁来写入文件，如果发生锁冲突，它就会被阻塞。环形缓冲区中的每个槽都有五个字段，分别是state、offset、size、ctime和checksum。校验和是前四个字段的哈希值。我们还在每个循环缓冲区的头部放置一个版本，以描述每个写操作的顺序。为了获得文件的锁，ulib首先使用原子的fetch_and_add来增加版本(①)。然后，它将一个锁项插入到环缓冲区的特定槽位中(②和③，这个位置由获取的版本号对环缓冲区大小取模确定)。当这个插槽与环形缓冲区的头部重叠时，插入被阻塞。在此之后，Ulib向后遍历循环缓冲区，以找到第一个冲突的锁项(即它们的写入数据重叠)。如果存在这样的冲突，Ulib会验证它的校验和，然后对它的状态进行轮询，直到它被释放(④)。如果线程在释放锁之前中止，Ulib还会反复检查它的ctime字段，以避免死锁。通过这种设计，多个线程可以在同一个文件中并发地写入不同的数据页。

## 3.4 三相写

一旦锁已经被获得，Ulib就可以实际写入文件数据。因为PM空间以只读模式映射到用户空间，所以Ulib不能直接写文件数据。相反，我们提出了一个三相写协议来执行直接写。为了确保崩溃的一致性，Kuco采用了copy-on-write (CoW)方法来写文件数据，其中新写入的数据总是重定向到新的PM页面。与NOVA和PMFS]类似，我们使用4KB作为默认的数据页面大小。Kuco中的写协议包括三个步骤。首先，Ulib通过两级锁定锁定文件，并向Kfs发送请求来分配新的PM页面。注意，通过使用CoW方式，覆盖和追加操作都需要空间分配。Kfs还需要修改相关的页表条目，以便在返回响应消息之前使这些分配的PM页面可写。其次，Ulib将未修改的数据从旧位置复制到用户缓冲区，并将新数据复制到已分配的PM页面，并通过flush指令将它们持久化。第三，Ulib向Kfs发送另一个请求来更新这个文件的元数据(即inode、块映射)，将新写的页面切换为只读，最后释放锁。

此外，我们引入了预分配机制，以避免为每个写操作从Kfs分配新的PM页。具体来说，我们允许Ulib从Kfs分配比预期更多的空闲页面(在我们的实现中，每次分配4 MB)。通过这种方式，Ulib可以使用本地空闲PM页面，而不需要与Kfs进行大多数写操作。当应用程序退出时，将未使用的页面返回给Kfs。对于异常退出，这些空闲页面暂时不能被其他应用程序重用，但仍然可以在恢复阶段回收(见§4.2)。预分配还有助于减少更新页表项的开销。当Kfs在每次分配后更新页表项时，它需要显式刷新相关的TLB项，以使修改可见。预分配允许一次分配多个数据页，因此TLB条目可以批量刷新。

## 3.5 版本化读取

在写协议中，由于CoW方式，新旧版本的数据页都被临时保留，这为我们提供了在不阻塞写操作的情况下读取数据的机会。但是，将逻辑文件映射到物理页面的块映射仍然由Kfs及时更新。这促使我们设计版本化的读机制来实现用户级的直接读，而不需要任何Kfs的参与，不管并发的写程序是什么。

![](/images/KucoFS/versioned_read_protocal.png)

版本化读取的设计目的是允许用户空间读取而不锁定文件，同时确保读取者不会从不完整的写入中读取数据。为了实现这一点，Kuco使用了一个类似于ext2的块映射来索引数据页，并在块映射的每个指针中嵌入一个版本字段。如图5所示，每个96位块映射项包含四个字段，分别是start、version、end和pointer。对于写操作，例如写3个数据页，Kfs按照如下格式更新对应的块映射项:1|V1|0|P1，0|V1|0|P2，0|V1|1|P3。特别是，这三个项目共享同一个版本(即V1)，这是由Ulib在获取范围锁时提供的(在3.3节中)。第一项的起始位和最后一项的结束位设置为1。我们只为指针字段保留40位，因为它指向一个4kb对齐的页面，较低的12位可以丢弃。

